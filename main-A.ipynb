{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>导入包</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T09:02:34.881795Z",
     "start_time": "2024-04-04T09:02:34.877788Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime, date\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "from xgboost import DMatrix\n",
    "import lightgbm as lgb\n",
    "from lightgbm import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from astral import LocationInfo\n",
    "from astral.sun import sunrise, sunset, dawn, noon, dusk\n",
    "from tqdm import tqdm\n",
    "\n",
    "from colorama import Fore, Style\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "is_debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>读取数据</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CFG   \n",
    "n_splits = 5   \n",
    "seed = 308    \n",
    "# root_dir = '/kaggle/input/child-mind-institute-problematic-internet-use/'\n",
    "example_dir = 'D:\\\\projects\\\\Kaggle\\\\CMI24\\\\dataset\\\\child-mind-institute-problematic-internet-use\\\\sample_submission.csv'  \n",
    "\n",
    "# meta_data:\n",
    "train_meta_dir = 'D:\\\\projects\\\\Kaggle\\\\CMI24\\\\dataset\\\\child-mind-institute-problematic-internet-use\\\\train.csv'\n",
    "test_meta_dir = 'D:\\\\projects\\\\Kaggle\\\\CMI24\\\\dataset\\\\child-mind-institute-problematic-internet-use\\\\test.csv'\n",
    "\n",
    "# time_series:\n",
    "train_ts_dir = 'D:\\\\projects\\\\Kaggle\\\\CMI24\\\\dataset\\\\child-mind-institute-problematic-internet-use\\\\series_train.parquet'\n",
    "test_ts_dir = 'D:\\\\projects\\\\Kaggle\\CMI24\\\\dataset\\\\child-mind-institute-problematic-internet-use\\\\series_test.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T09:02:35.261961Z",
     "start_time": "2024-04-04T09:02:34.883687Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T09:02:35.452976Z",
     "start_time": "2024-04-04T09:02:35.263960Z"
    }
   },
   "outputs": [],
   "source": [
    "def printcolor(text):\n",
    "    print(Fore.YELLOW + text + Style.RESET_ALL)\n",
    "\n",
    "def process_file(folder_path, folder_name):\n",
    "    try:\n",
    "        file_path = os.path.join(folder_path, \"part-0.parquet\")\n",
    "        \n",
    "        # 读取 parquet 文件\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        # 提取 ID 信息，并将其添加为新列\n",
    "        file_id = folder_name.split('=')[1]\n",
    "        df['id'] = file_id\n",
    "        \n",
    "        # 删除步长列\n",
    "        if 'step' in df.columns:\n",
    "            df = df.drop(columns=['step'])\n",
    "        \n",
    "        # 添加时间相关特征\n",
    "        df['time_of_day_hours'] = df['time_of_day'] / 1e9 / 3600  # 将纳秒转换为小时\n",
    "        df['day_time'] = df['relative_date_PCIAT'] + (df['time_of_day_hours'] / 24)\n",
    "        df['day_of_week'] = (df['relative_date_PCIAT'] % 7).astype(int)  # 将日期映射为星期几\n",
    "        df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)  # 标记周末\n",
    "        \n",
    "        # 将时间特征放在 DataFrame 最前面\n",
    "        cols = ['time_of_day_hours', 'day_time'] + [col for col in df.columns if col not in ['time_of_day_hours', 'day_time','time_of_day']]\n",
    "        df = df[cols]\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {folder_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_ts(root_dir):\n",
    "    data = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # 获取所有 'id=...' 文件夹路径\n",
    "        folder_paths = [\n",
    "            (os.path.join(root_dir, folder_name), folder_name)\n",
    "            for folder_name in os.listdir(root_dir)\n",
    "            if os.path.isdir(os.path.join(root_dir, folder_name)) and folder_name.startswith(\"id=\")\n",
    "        ]\n",
    "        \n",
    "        # 在调试模式下限制为只读取前五个文件夹\n",
    "        if is_debug:\n",
    "            folder_paths = folder_paths[:1]\n",
    "            printcolor(\"Debug Mode: Only loaded 5 training data files\")\n",
    "        # 提交所有文件的处理任务\n",
    "        futures = {executor.submit(process_file, path, name): name for path, name in folder_paths}\n",
    "        \n",
    "        # 使用 tqdm 显示进度条\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Processing files in {root_dir}\"):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                data.append(result)\n",
    "    \n",
    "    # 合并所有 DataFrame\n",
    "    return pd.concat(data, ignore_index=True)\n",
    "\n",
    "def plot_features_over_time(df, time_column='day_time'):\n",
    "    features = [col for col in df.columns if col not in [time_column, 'id']]\n",
    "    \n",
    "    num_features = len(features)\n",
    "    fig, axes = plt.subplots(nrows=num_features, ncols=1, figsize=(10, num_features * 3), sharex=True)\n",
    "    fig.suptitle(\"Features Over Time\", fontsize=16)\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        axes[i].plot(df[time_column], df[feature], label=feature)\n",
    "        axes[i].set_ylabel(feature)\n",
    "        axes[i].legend(loc='upper right')\n",
    "    \n",
    "    axes[-1].set_xlabel(time_column)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "def feature_engineering(df):\n",
    "    season_cols = [col for col in df.columns if 'Season' in col]\n",
    "    df = df.drop(season_cols, axis=1) \n",
    "    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n",
    "    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n",
    "    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n",
    "    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n",
    "    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n",
    "    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n",
    "    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n",
    "    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n",
    "    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n",
    "    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n",
    "    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n",
    "    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n",
    "    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n",
    "    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n",
    "    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim, hidden_dims=None):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        # 编码器层\n",
    "        encoder_layers = []\n",
    "        current_dim = input_dim\n",
    "        if hidden_dims:\n",
    "            for hidden_dim in hidden_dims:\n",
    "                encoder_layers.append(nn.Linear(current_dim, hidden_dim))\n",
    "                encoder_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "                encoder_layers.append(nn.LeakyReLU())\n",
    "                current_dim = hidden_dim\n",
    "        encoder_layers.append(nn.Linear(current_dim, encoding_dim))\n",
    "        encoder_layers.append(nn.ReLU())\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # 解码器层\n",
    "        decoder_layers = []\n",
    "        current_dim = encoding_dim\n",
    "        for hidden_dim in reversed(hidden_dims or []):\n",
    "            decoder_layers.append(nn.Linear(current_dim, hidden_dim))\n",
    "            decoder_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            decoder_layers.append(nn.LeakyReLU())\n",
    "            current_dim = hidden_dim\n",
    "        decoder_layers.append(nn.Linear(current_dim, input_dim))\n",
    "        decoder_layers.append(nn.Sigmoid())\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "def perform_autoencoder(df, encoding_dim=50, hidden_dims=[128, 64], epochs=300, batch_size=64, patience=15):\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df)\n",
    "    data_tensor = torch.FloatTensor(df_scaled)\n",
    "    \n",
    "    input_dim = data_tensor.shape[1]\n",
    "    autoencoder = AutoEncoder(input_dim, encoding_dim, hidden_dims)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "    \n",
    "    # 早停机制和保存最优模型\n",
    "    best_loss = np.inf\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for i in range(0, len(data_tensor), batch_size):\n",
    "            batch = data_tensor[i : i + batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = autoencoder(batch)\n",
    "            loss = criterion(reconstructed, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        epoch_loss /= (len(data_tensor) / batch_size)\n",
    "        \n",
    "        # 输出损失\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}')\n",
    "        \n",
    "        # 早停机制：如果损失没有下降，增加计数器\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = autoencoder.state_dict()  # 保存最佳模型\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # 如果超过耐心次数，停止训练\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "    \n",
    "    # 加载最佳模型\n",
    "    if best_model_state:\n",
    "        autoencoder.load_state_dict(best_model_state)\n",
    "    \n",
    "    # 提取编码后的数据\n",
    "    with torch.no_grad():\n",
    "        encoded_data = autoencoder.encoder(data_tensor).numpy()\n",
    "        \n",
    "    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_ts = load_ts(train_ts_dir)  \n",
    "test_ts = load_ts(test_ts_dir)\n",
    "\n",
    "train_meta = pd.read_csv(train_meta_dir)\n",
    "test_meta = pd.read_csv(test_meta_dir)\n",
    "\n",
    "df_train = train_ts.drop('id', axis=1)\n",
    "df_test = test_ts.drop('id', axis=1)\n",
    "\n",
    "train_ts_encoded = perform_autoencoder(df_train, encoding_dim=60, epochs=100, batch_size=32)\n",
    "test_ts_encoded = perform_autoencoder(df_test, encoding_dim=60, epochs=100, batch_size=32)\n",
    "\n",
    "time_series_cols = train_ts_encoded.columns.tolist()\n",
    "train_ts_encoded[\"id\"]=train_ts[\"id\"]\n",
    "test_ts_encoded['id']=test_ts[\"id\"]\n",
    "\n",
    "train = pd.merge(train_meta, train_ts_encoded, how=\"left\", on='id')\n",
    "test = pd.merge(test_meta, test_ts_encoded, how=\"left\", on='id')\n",
    "column_types = train_ts.dtypes\n",
    "print(column_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types = train_ts.dtypes\n",
    "print(column_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 滑窗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "window_size = 10\n",
    "# 创建一个新的 DataFrame 来存储统计特征\n",
    "stat_features = pd.DataFrame()\n",
    "# 对需要提取统计特征的列进行处理，假设 'X', 'Y', 'Z' 为主要时间序列特征\n",
    "columns_to_drop = ['X', 'Y', 'Z']  # 原始列\n",
    "for col in [columns_to_drop]:\n",
    "    # 滑动均值\n",
    "    stat_features[f'{col}_mean'] = train_ts[col].rolling(window=window_size).mean()\n",
    "    # 滑动标准差\n",
    "    stat_features[f'{col}_std'] = train_ts[col].rolling(window=window_size).std()\n",
    "    # 滑动最大值\n",
    "    stat_features[f'{col}_max'] = train_ts[col].rolling(window=window_size).max()\n",
    "    # 滑动最小值\n",
    "    stat_features[f'{col}_min'] = train_ts[col].rolling(window=window_size).min()\n",
    "    # 滑动和\n",
    "    stat_features[f'{col}_sum'] = train_ts[col].rolling(window=window_size).sum()\n",
    "    # 滑动中位数\n",
    "    stat_features[f'{col}_median'] = train_ts[col].rolling(window=window_size).median()\n",
    "\n",
    "train_ts_processed = train_ts.drop(columns=columns_to_drop)\n",
    "train_ts = pd.concat([train_ts, stat_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=5)\n",
    "numeric_cols = train.select_dtypes(include=['float64', 'int64']).columns\n",
    "imputed_data = imputer.fit_transform(train[numeric_cols])\n",
    "train_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n",
    "train_imputed['sii'] = train_imputed['sii'].round().astype(int)\n",
    "for col in train.columns:\n",
    "    if col not in numeric_cols:\n",
    "        train_imputed[col] = train[col]\n",
    "\n",
    "\n",
    "train = train_imputed\n",
    "\n",
    "train = feature_engineering(train)\n",
    "train = train.dropna(thresh=10, axis=0)\n",
    "test = feature_engineering(test)\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test  = test .drop('id', axis=1)   \n",
    "\n",
    "\n",
    "featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-CGAS_Score', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n",
    "                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n",
    "                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW']\n",
    "\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "train = train[featuresCols]\n",
    "train = train.dropna(subset='sii')\n",
    "\n",
    "featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-CGAS_Score', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n",
    "                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n",
    "                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW']\n",
    "\n",
    "featuresCols += time_series_cols\n",
    "test = test[featuresCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# df = test_ts\n",
    "\n",
    "# # 获取所有的 id\n",
    "# unique_ids = df['id'].unique()\n",
    "\n",
    "# # 选择一些要绘制的特征列\n",
    "# features = ['X', 'Y', 'Z', 'enmo', 'anglez', 'light', 'battery_voltage']\n",
    "\n",
    "# # 设置图表大小\n",
    "# fig, axes = plt.subplots(len(features), 1, figsize=(12, 3 * len(features)), sharex=True)\n",
    "# fig.suptitle(\"Feature Variation over day_time for each ID\")\n",
    "\n",
    "# # 对每个特征绘制图表\n",
    "# for feature, ax in zip(features, axes):\n",
    "#     for uid in unique_ids:\n",
    "#         # 提取当前 id 的数据\n",
    "#         df_id = df[df['id'] == uid]\n",
    "#         ax.plot(df_id['day_time'], df_id[feature], label=f'ID {uid}')\n",
    "    \n",
    "#     # 设置标签和标题\n",
    "#     ax.set_ylabel(feature)\n",
    "#     ax.legend(loc=\"upper right\", fontsize='small')\n",
    "#     ax.grid(True)\n",
    "\n",
    "# # 设置 x 轴标签\n",
    "# axes[-1].set_xlabel(\"day_time\")\n",
    "# plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T09:03:06.543088Z",
     "start_time": "2024-04-04T09:03:06.448070Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T09:03:06.575089Z",
     "start_time": "2024-04-04T09:03:06.560089Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>训练模型</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>评测指标</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T09:03:08.521266Z",
     "start_time": "2024-04-04T09:03:08.507267Z"
    }
   },
   "outputs": [],
   "source": [
    "if np.any(np.isinf(train)):\n",
    "    train = train.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LGB</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def TrainML(model_class, test_data, train_data, n_splits=5, use_group_kfold=False, optimize_method='Powell'):\n",
    "    # 提取训练集特征和标签\n",
    "    X = train_data.drop(['sii', 'group_column'], axis=1)  # 假设 group_column 是用于 GroupKFold 的列\n",
    "    y = train_data['sii']\n",
    "    \n",
    "    # 初始化交叉验证方法\n",
    "    if use_group_kfold:\n",
    "        cv = GroupKFold(n_splits=n_splits)\n",
    "        groups = train_data['group_column']  # 假设每个样本都有一个 group_column\n",
    "    else:\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "        groups = None\n",
    "    \n",
    "    # 初始化预测结果和得分列表\n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float)\n",
    "    oof_rounded = np.zeros(len(y), dtype=int)\n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "    train_scores, val_scores = [], []\n",
    "\n",
    "    # 交叉验证训练\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y, groups=groups)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        # 保存预测结果\n",
    "        oof_non_rounded[val_idx] = y_val_pred\n",
    "        oof_rounded[val_idx] = np.round(y_val_pred).astype(int)\n",
    "        \n",
    "        train_kappa = quadratic_weighted_kappa(y_train, np.round(y_train_pred).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, oof_rounded[val_idx])\n",
    "\n",
    "        train_scores.append(train_kappa)\n",
    "        val_scores.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "\n",
    "    print(f\"Mean Train QWK: {np.mean(train_scores):.4f}\")\n",
    "    print(f\"Mean Validation QWK: {np.mean(val_scores):.4f}\")\n",
    "    \n",
    "    # 可视化评估\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, n_splits + 1), train_scores, label=\"Train QWK\")\n",
    "    plt.plot(range(1, n_splits + 1), val_scores, label=\"Validation QWK\")\n",
    "    plt.xlabel(\"Fold\")\n",
    "    plt.ylabel(\"Quadratic Weighted Kappa\")\n",
    "    plt.title(\"QWK Scores per Fold\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # 优化阈值\n",
    "    def evaluate_predictions(thresholds, y_true, y_pred):\n",
    "        y_pred_tuned = threshold_Rounder(y_pred, thresholds)\n",
    "        return -quadratic_weighted_kappa(y_true, y_pred_tuned)\n",
    "    \n",
    "    KappaOptimizer = minimize(evaluate_predictions, x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), method=optimize_method)\n",
    "    assert KappaOptimizer.success, \"Optimization did not converge.\"\n",
    "\n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOptimizer.x)\n",
    "    tuned_kappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "    print(f\"Optimized QWK Score: {tuned_kappa:.4f}\")\n",
    "    \n",
    "    # 生成提交文件\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tp_tuned = threshold_Rounder(tpm, KappaOptimizer.x)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_data['id'],  # 假设 test_data 中包含 'id' 列\n",
    "        'sii': tp_tuned\n",
    "    })\n",
    "    \n",
    "    return submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voting_model = VotingRegressor(estimators=[\n",
    "#     ('lightgbm', Light)\n",
    "    # ('xgboost', XGB_Model),\n",
    "    # ('catboost', CatBoost_Model),\n",
    "    # ('tabnet', TabNet_Model)\n",
    "# ])\n",
    "\n",
    "submission1 = TrainML(LGB, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T09:03:08.536266Z",
     "start_time": "2024-04-04T09:03:08.522267Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>xgboost模型</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>交叉验证</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T09:06:02.872038Z",
     "start_time": "2024-04-04T09:03:08.553112Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T09:06:02.888038Z",
     "start_time": "2024-04-04T09:06:02.873038Z"
    }
   },
   "outputs": [],
   "source": [
    "# importance = DataFrame()\n",
    "# importance[\"特征\"] = model_lgb[0].feature_name()\n",
    "# importance[\"重要性\"] = 0\n",
    "# for model in model_lgb:\n",
    "#     importance[\"重要性\"] = importance[\"重要性\"] + model.feature_importance()\n",
    "# importance[\"重要性\"] = importance[\"重要性\"] / kfold.n_splits\n",
    "# importance.sort_values(\"重要性\", ascending=False)[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>预测</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T09:06:04.032159Z",
     "start_time": "2024-04-04T09:06:02.889038Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T09:06:04.160160Z",
     "start_time": "2024-04-04T09:06:04.033159Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T09:06:04.192160Z",
     "start_time": "2024-04-04T09:06:04.161160Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T09:06:04.256160Z",
     "start_time": "2024-04-04T09:06:04.193160Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "211.52px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "244.83px",
    "left": "1957.27px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
